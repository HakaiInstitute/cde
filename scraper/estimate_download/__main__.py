import pandas as pd
import numpy as np

from sqlalchemy import create_engine, types
import configparser
import argparse

config = configparser.ConfigParser()
config.read(".env")
envs = config["scheduler"]
database_link = f"postgresql://{envs['DB_USER']}:{envs['DB_PASSWORD']}@{envs['DB_HOST']}:5432/{envs['DB_NAME']}"

engine = create_engine(database_link)


def query_profiles(query):
    """
    Query the database for subsets matching the query generated by CEDA interface
    """
    # Generate Query to database
    dateformat = "%Y-%m-%dT%H:%M:%SZ"
    sql_query = f"""
    SELECT * FROM profiles WHERE(
        ST_Contains(ST_GeomFromText({query['polygon_region']},4326),ST_Transform(geom,4326)) is true and
        (erddap_url,dataset_id) in {tuple((x['erddap_url'],x['dataset_id']) for x in query['cache_filtered'])}
        time_min<'{query['time_max']}' and 
        time_max>'{query['time_min']}' and  
        depth_min<{query['depth_max']} and  
        depth_max>{query['depth_min']}
        )"""

    # Retrieve query result
    return pd.read_sql(sql_query, con=engine, index_col="pk")


def estimate_n_record_per_profile(profiles, query):
    """
    Estimate the amount of records expected based on the
    record count and record density along and z variables.
    """

    # Convert time variables to datetime
    for time_var in ["time_min", "time_max"]:
        profiles[time_var] = pd.to_datetime(profiles[time_var], utc=True)

    # Derive Profiles Densities
    dz = profiles["depth_max"] - profiles["depth_min"]
    dt = (profiles["time_max"] - profiles["time_min"]).dt.days
    profiles["rec_per_day"] = profiles["n_profiles"] / dt.where(dt > 0, np.nan).fillna(
        profiles["n_records"] / dt.where(dt > 0, np.nan)
    )
    profiles["rec_per_m_vert"] = (
        profiles["n_records"] / profiles["n_profiles"]
    ) / dz.where(dz > 0, np.nan)

    # Reduce limits to the narrowest between the query and subset limits
    time_min = profiles["time_min"].where(
        profiles["time_min"] > query["time_min"], query["time_min"]
    )
    time_max = profiles["time_max"].where(
        profiles["time_max"] < query["time_max"], query["time_max"]
    )
    depth_min = profiles["depth_min"].where(
        profiles["depth_min"] > query["depth_min"], query["depth_min"]
    )
    depth_max = profiles["depth_max"].where(
        profiles["depth_max"] < query["depth_max"], query["depth_max"]
    )

    # Estimate amount of records expected
    # First estimate vertical records expected then horizontal and multiply both
    z_records = (depth_max - depth_min) * profiles["rec_per_m_vert"]
    t_records = (time_max - time_min).dt.days * profiles["rec_per_day"]
    profiles["estimated_n_records"] = (
        (z_records * t_records).fillna(z_records).fillna(t_records)
    )
    return profiles


def estimate_query_size_per_dataset(query):
    """
    General wrapper that interact with the different components to estimate a query download size.
    """
    # Regroup by dataset
    profiles = query_profiles(query["user_query"])

    # Estimate records per profiles
    profiles = estimate_n_record_per_profile(profiles, query["user_query"])

    # Sum all subsets records by datasets
    datasets = (
        profiles.groupby(["erddap_url", "dataset_id"])["estimated_n_records"]
        .sum()
        .to_frame()
    )
    return datasets


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("query")
    args = parser.parse_args()
    estimate_query_size_per_dataset(args.erddap_urls, args.csv_only)
